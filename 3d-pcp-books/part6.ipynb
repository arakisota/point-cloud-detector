{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0261c4b-dd1c-4228-99b6-e7811c9e6987",
   "metadata": {},
   "source": [
    "# 第6章 深層学習による3次元点群処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2358368-9338-4b87-8a43-fcb5fe8040b7",
   "metadata": {},
   "source": [
    "## 6.1 深層学習の基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e87c98c-d7d8-43c2-b357-c3183ff06594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#入力は128次元のベクトル\n",
    "input_data = torch.rand([128])\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e8549c-7e37-4c00-acaf-53ae90aa9662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "#入力128次元、出力256次元\n",
    "linear = nn.Linear(128, 256)\n",
    "input_data = torch.zeros((128))\n",
    "\n",
    "x = input_data\n",
    "#線形変換\n",
    "x = linear(x)\n",
    "#活性化関数はReLU\n",
    "x = F.relu(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5e3914-2a19-4806-b377-83313a98abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#MLP : 多層パーセプトロンの実装\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(128, 256)\n",
    "        self.linear2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "net = MLP()\n",
    "y = net(input_data)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64fd789-5192-4aa3-88f7-1861a0063cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "nrt = nn.Sequential(nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "y = net(input_data)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ede48ab-3946-41ab-9ed5-1abbf0a4f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "#ミニバッチの要素数×入力ベクトルの次元\n",
    "input_data = torch.rand([32, 128])\n",
    "y = net(input_data)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b444d2-3e4a-41f7-b9e4-9e7448e53284",
   "metadata": {},
   "source": [
    "### Dockerができなかった"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdd3d4-0087-4dcb-9b1e-efb21396eb2e",
   "metadata": {},
   "source": [
    "## 6.2 Pytorch Geometricによる3次元点群の扱い\n",
    "- Pytorch Geometric \n",
    "    - 点群やメッシュをグラフとして出力する\n",
    "        - 頂点の位置情報 : position\n",
    "        - 各頂点の特徴量 : x\n",
    "        - 各頂点の法線 : normal\n",
    "        - 辺の張られ方 : edge_index\n",
    "        - 各辺の特徴量 : edge_attr\n",
    "        - 面の張られ方 : face\n",
    "        - グラフ全体の特徴量・物体カテゴリ : y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf07b46-2ebb-4381-9198-acbed2b6c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading http://vision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
      "Extracting /Users/sotaaraki/3dpcp_book_codes/notebook/modelnet10/ModelNet10.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Pytorch GeimetricのModelNetデータセット機能を使う。ダウンロードして、前処理してデータを保存\n",
    "from pathlib import Path\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#一時的に保存するディレクトリを指定\n",
    "current_path = Path.cwd()\n",
    "dataset_dir = current_path / \"modelnet10\"\n",
    "\n",
    "#点群に適用する前処理を指定\n",
    "pre_transform = T.Compose([\n",
    "    T.SamplePoints(1024, remove_faces=True, include_normals=True),\n",
    "    T.NormalizeScale(),\n",
    "])\n",
    "\n",
    "train_dataset = ModelNet(dataset_dir, name=\"10\", train=True, transform=None,\n",
    "                        pre_transform=pre_transform, pre_filter=None)\n",
    "test_dataset = ModelNet(dataset_dir, name=\"10\", train=False, transform=None,\n",
    "                       pre_transform=pre_transform, pre_filter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4cc5d7-3014-4c31-8765-c5bf7e5eea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len :  3991\n",
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset len : \", len(train_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d9b0b2-c4ef-4aa7-8455-de0026f77e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3])\n",
      "tensor([[-0.3016, -0.0504, -0.1324],\n",
      "        [-0.3738,  0.0773, -0.0737],\n",
      "        [-0.4516, -0.3701,  0.2578],\n",
      "        ...,\n",
      "        [ 0.2526,  0.1295, -0.2235],\n",
      "        [ 0.4451,  0.4263,  0.1332],\n",
      "        [ 0.1182, -0.3978, -0.2363]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].pos.shape)\n",
    "print(train_dataset[0].pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99210024-6023-4deb-a369-4c54cd6af49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(pos=[32768, 3], y=[32], normal=[32768, 3], batch=[32768], ptr=[33])\n"
     ]
    }
   ],
   "source": [
    "#ミニバッチとして学習するために、Batch型を準備する\n",
    "#各データが1024点からなる点群を32個にまとめてミニバッチしたため、1024×32 = 32768点含まれている\n",
    "from torch_geometric.data import DataLoader as DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4d8d1-eaea-4239-a3ff-1451c7ea0ce7",
   "metadata": {},
   "source": [
    "## 6.3 PointNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962781e7-2265-4399-9f73-34244b403556",
   "metadata": {},
   "source": [
    "PointNet\n",
    "- 3次元点群\n",
    "    - 順不同なデータ\n",
    "        - symmetric Functionでうまく扱える\n",
    "            - 入力データの順番が変わったとしても、出力が変わらないような関数を指す\n",
    "- Shared MLPとMax-Poolingを組み合わせたネットワークを提案している\n",
    "    - shared MLP\n",
    "        - 各点について、チャンネル方向に同一のMLPを適用する手法\n",
    "        - 画像だとPointwise Convolution, 1×1 Convolutionと呼ばれている構造\n",
    "- Shared MLPの後は、Max-Poolingで全点の特徴量を集約する\n",
    "    - これは多視点画像の処理において、各視点で計算した特徴量を全ての視点について集約するView-Poolingと同じアイデア\n",
    "    - これはチャンネルごとに適用される\n",
    "    - Maxを取ることで、点の順序によらない出力を得ることができる\n",
    "        - 他にもMin, averageなどの方法がある\n",
    "        - 順序によらないプーリング操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "025dde1d-3580-44c7-aa92-2bee73f233e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch_geometric.datasets import ModelNet\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "current_path = Path.cwd()\n",
    "dataset_dir = current_path / \"modelnet10\"\n",
    "\n",
    "pre_transform = T.Compose([\n",
    "    T.SamplePoints(1024, remove_faces=True, include_normals=True),\n",
    "    T.NormalizeScale(),\n",
    "])\n",
    "\n",
    "train_dataset = ModelNet(dataset_dir, name=\"10\", train=True, transform=None, pre_transform=pre_transform, pre_filter=None)\n",
    "test_dataset = ModelNet(dataset_dir, name=\"10\", train=False, transform=None, pre_transform=pre_transform, pre_filter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff2da8d8-abe8-41ba-9a96-4136386a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 3991\n",
      "Data(pos=[1024, 3], y=[1], normal=[1024, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset len:\", len(train_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9bda9f6-5ca8-4da8-a272-68a6fb998603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3])\n",
      "tensor([[-0.3016, -0.0504, -0.1324],\n",
      "        [-0.3738,  0.0773, -0.0737],\n",
      "        [-0.4516, -0.3701,  0.2578],\n",
      "        ...,\n",
      "        [ 0.2526,  0.1295, -0.2235],\n",
      "        [ 0.4451,  0.4263,  0.1332],\n",
      "        [ 0.1182, -0.3978, -0.2363]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].pos.shape)\n",
    "print(train_dataset[0].pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deeac6d-689e-4381-8f2b-f0d9f2494fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b63b4860-6938-4955-8428-9d35a42b8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(pos=[32768, 3], y=[32], normal=[32768, 3], batch=[32768], ptr=[33])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "num_graphs : batch内の点群数\n",
    "torch.size : 点群数×出力特徴量の次元\n",
    "\"\"\"\n",
    "#Symmetric functionの実装\n",
    "from torch_geometric.nn import global_max_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "#Pytorchによるモデルを継承してる\n",
    "class SymmFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        #Shaered MLPを定義している\n",
    "        super(SymmFunction, self).__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "        nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "        nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "        nn.Linear(128, 512)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        #点ごとの特徴量の変換を表している\n",
    "        x = self.shared_mlp(batch.pos)\n",
    "        #みにバッチ中に含まれる点群単位でプーリングを行っている\n",
    "        x = global_max_pool(x, batch.batch)\n",
    "        return x\n",
    "    \n",
    "f = SymmFunction()\n",
    "print(batch)\n",
    "y = f(batch)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341008af-cbed-4e33-b0d8-05d33eab6087",
   "metadata": {},
   "source": [
    "- T-Net\n",
    "    - Spatial Transformer Network(STN)のアイデアを導入したモジュール\n",
    "    - 入力された点群の回転を学習ベースで自動的に正規化することを目指して導入される\n",
    "- 回転の正規化の流れ\n",
    "    - Shaered MLPとMax-Poolingで特徴量を取り出し、さらにMLPで9次元ベクトルにする\n",
    "    - この9次元ベクトルを3×3の回転行列だと考えて、入力点群全体にこの回転を適用する\n",
    "        - 回転行列を作用させる\n",
    "    - ネットワークの出力が回転行列そのものでなく、回転行列と単位行列の差分となるようにしてる\n",
    "        - これによって、学習する要素は単位行列からの変化分のみをネットワークが出力すれば良い\n",
    "        - ネットワークの負担が軽くなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd857a0-08c8-4e07-b44e-81e836f03ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#回転行列の正規化\n",
    "class InputTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        #Max-Poolingで1024次元のベクトルに変換\n",
    "        super(InputTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "        nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "        nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "        nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU()\n",
    "        )\n",
    "        #MLPで9次元まで変換する\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 9)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pooling(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        #9次元を3×3の回転行列にする\n",
    "        x = x.view(-1, 3, 3)\n",
    "        id_matrix = torch.eye(3).to(x.device).view(1, 3, 3).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fb235-e602-455d-ba90-4ce46ccdb9bd",
   "metadata": {},
   "source": [
    "- 特徴量に行列を作用させる\n",
    "    - これは今の特徴量空間から変換先の特徴量空間への線型写像に対応する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6318bded-908f-41f0-80fc-673ab6791794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureTNet, self).__init__()\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU()\n",
    "        )\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "            nn.Linear(256, 64*64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        x = self.input_mlp(x)\n",
    "        x = global_max_pooling(x, batch)\n",
    "        x = self.output_mlp(x)\n",
    "        x = x.view(-1, 64, 64)\n",
    "        id_matrix = torch.eye(64).to(x.device).view(1, 64, 64).repeat(x.shape[0], 1, 1)\n",
    "        x = id_matrix + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b33f2-2510-493e-9661-a3455eb01be7",
   "metadata": {},
   "source": [
    "### PointNetによるクラス分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8ca603d-47ea-4fbf-8e58-5d46e137c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetClassification, self).__init__()\n",
    "        #入力点群に対するT-Net\n",
    "        self.inupt_tnet = InputTNet()\n",
    "        #点ごとに特徴量を変換するShared MLPが二つ(mlp1, mlp2)\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "        )\n",
    "        #特徴量空間でのT-Net\n",
    "        self.feature_tnet = FeatureTNet()\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU()\n",
    "        )\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(p=0.3), \n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(p=0.3), \n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        x = batch_data.pos\n",
    "        \n",
    "        #入力点群に対するT-Netによって、入力点群に対した回転行列を計算する\n",
    "        input_transform = self.input_tnet(x, batch_data.batch)\n",
    "        #回転行列は入力点群それぞれについて計算されており、結合されてTensorに格納されているため、それぞれ展開する\n",
    "        #batch_data.batch : どの点はどの点群に属しているのか\n",
    "        transform = input_transform[batch_data.batch, :, :]\n",
    "        #bmm : batch matrix mutliplication\n",
    "        x = torch.bmm(transform, x.view(-1 ,3, 1)).view(-1, 3)\n",
    "        \n",
    "        #正規化された点群をShared MLPに入力し、64次元の特徴量を得る\n",
    "        x = self.mlp1(x)\n",
    "        \n",
    "        #これまで得られた、特徴量を特徴量空間でのT-Netに入力し、特徴量空間で作用させるための行列を計算する\n",
    "        feature_transform = self.feature_tnet(x, batch_data.batch)\n",
    "        transform = feature_transform[batch_data.batch, :, :]\n",
    "        x = torch.bmm(transform, x.view(-1, 64))\n",
    "        \n",
    "        x = self.mlp2(x)\n",
    "        x = global_max_pool(x, batch_data.batch)\n",
    "        x = self.mlp3(x)\n",
    "        \n",
    "        return x, input_transform, feature_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79e02dc0-c0aa-4906-ae63-d3b5b10f6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sotaaraki/opt/anaconda3/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#学習\n",
    "import torch \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "num_epoch = 400\n",
    "batch_size = 32\n",
    "\n",
    "#CUDA環境でGPUができるか確認\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#ここまでに実装したモデルを生成\n",
    "model = PointNetClassification()\n",
    "#デバイスに転送\n",
    "model = model.to(device)\n",
    "\n",
    "#optimizer関連の設定をしている\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epoch // 4, gamma=0.5)\n",
    "\n",
    "log_dir = current_path / \"log_modelnet10_classification\"\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "train_dateloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab57d37a-260e-424a-ad58-bdef6e05b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e02d2004a510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mthis_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "#学習ループ全体\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_data in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        batch_data = batch_data.to(device)\n",
    "        this_batch_size = batch_data.batch.detach().max() + 1\n",
    "        \n",
    "        pred_y, _, feature_transform = model(batch_data)\n",
    "        true_y = batch_data.y.detach()\n",
    "\n",
    "        class_loss = criteria(pred_y, true_y)\n",
    "        accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "        id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "        transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "        reg_loss = transform_norm.mean()\n",
    "\n",
    "        loss = class_loss + reg_loss * 0.001\n",
    "        \n",
    "        losses.append({\n",
    "            \"loss\": loss.item(),\n",
    "            \"class_loss\": class_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"seen\": float(this_batch_size)})\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        model_path = log_dir / f\"model_{epoch:06}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    loss = 0\n",
    "    class_loss = 0\n",
    "    reg_loss = 0\n",
    "    accuracy = 0\n",
    "    seen = 0\n",
    "    for d in losses:\n",
    "        seen = seen + d[\"seen\"]\n",
    "        loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "        class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "        reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "        accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "    loss = loss / seen\n",
    "    class_loss = class_loss / seen\n",
    "    reg_loss = reg_loss / seen\n",
    "    accuracy = accuracy / seen\n",
    "    writer.add_scalar(\"train_epoch/loss\", loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/class_loss\", class_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/reg_loss\", reg_loss, epoch)\n",
    "    writer.add_scalar(\"train_epoch/accuracy\", accuracy, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "\n",
    "        losses = []\n",
    "        for batch_data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            batch_data = batch_data.to(device)\n",
    "            this_batch_size = batch_data.batch.detach().max() + 1\n",
    "\n",
    "            pred_y, _, feature_transform = model(batch_data)\n",
    "            true_y = batch_data.y.detach()\n",
    "\n",
    "            class_loss = criteria(pred_y, true_y)\n",
    "            accuracy =float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
    "\n",
    "            id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
    "            transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
    "            reg_loss = transform_norm.mean()\n",
    "\n",
    "            loss = class_loss + reg_loss * 0.001\n",
    "\n",
    "            losses.append({\n",
    "                \"loss\": loss.item(),\n",
    "                \"class_loss\": class_loss.item(),\n",
    "                \"reg_loss\": reg_loss.item(),\n",
    "                \"accuracy\": accuracy,\n",
    "                \"seen\": float(this_batch_size)})\n",
    "            \n",
    "        loss = 0\n",
    "        class_loss = 0\n",
    "        reg_loss = 0\n",
    "        accuracy = 0\n",
    "        seen = 0\n",
    "        for d in losses:\n",
    "            seen = seen + d[\"seen\"]\n",
    "            loss = loss + d[\"loss\"] * d[\"seen\"]\n",
    "            class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
    "            reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
    "            accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
    "        loss = loss / seen\n",
    "        class_loss = class_loss / seen\n",
    "        reg_loss = reg_loss / seen\n",
    "        accuracy = accuracy / seen\n",
    "        writer.add_scalar(\"test_epoch/loss\", loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/class_loss\", class_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/reg_loss\", reg_loss, epoch)\n",
    "        writer.add_scalar(\"test_epoch/accuracy\", accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139e5e4-aff1-40a0-a98a-93eb77e6577f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60005f5e-8844-4404-bbd9-15bcbb144efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66257c-6772-4d18-8840-bc4f2f0d8e50",
   "metadata": {},
   "source": [
    "## 点群の畳み込み\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cf3d8-104c-4633-a9be-ea46f58857ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
